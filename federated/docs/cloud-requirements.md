## Requirements for Running Federated Swarm on Cloud Platform

Fderated Swarm's job execution engine consists of the following required components:

1. Orchestration
In order to manage different workflow for Federated Swarm's Downsampling and Profiler, a job scheduler or workflow engine
is required. The following services are used in the core providers.
- GCP: Managed by dsub
- AWS: Managed by AWS Batch
- Azure: Managed by Azure Batch

These services allow for planning, scheduling, execution, and real-time monitoring of batch workloads across a range of compute types.

2. Compute
Federated Swarm components require Linux compute nodes to run stages in a workflow.
The compute nodes can vary in terms of CPU, Memory, Storage, and GPU in order to accommodate the workload.

3. Storage
Federated Swarm components require block storage in order to read and persist information that can be shared across stages in the workflow.
The following services are used in the core providers:
- GCP: Google Cloud Storage
- AWS: S3
- Azure: Blob Storage

The files that are read by Federated Swarm are as follows:
- Input files
- Reference files

The types of files generated by Federated Swarm are as follows:
- Job execution script
- Downsampling output
- Profiling results